{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FedericoSabbadini/DeepLearning/blob/main/AutoDifferentiation_TensorFlow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "daf8b31c",
      "metadata": {
        "origin_pos": 1,
        "id": "daf8b31c"
      },
      "source": [
        "# Automatic Differentiation\n",
        ":label:`sec_autograd`\n",
        "\n",
        "Recall from :numref:`sec_calculus`\n",
        "that calculating derivatives is the crucial step\n",
        "in all the optimization algorithms\n",
        "that we will use to train deep networks.\n",
        "While the calculations are straightforward,\n",
        "working them out by hand can be tedious and error-prone,\n",
        "and these issues only grow\n",
        "as our models become more complex.\n",
        "\n",
        "Fortunately all modern deep learning frameworks\n",
        "take this work off our plates\n",
        "by offering *automatic differentiation*\n",
        "(often shortened to *autograd*).\n",
        "As we pass data through each successive function,\n",
        "the framework builds a *computational graph*\n",
        "that tracks how each value depends on others.\n",
        "To calculate derivatives,\n",
        "automatic differentiation\n",
        "works backwards through this graph\n",
        "applying the chain rule.\n",
        "The computational algorithm for applying the chain rule\n",
        "in this fashion is called *backpropagation*.\n",
        "\n",
        "While autograd libraries have become\n",
        "a hot concern over the past decade,\n",
        "they have a long history.\n",
        "In fact the earliest references to autograd\n",
        "date back over half of a century :cite:`Wengert.1964`.\n",
        "The core ideas behind modern backpropagation\n",
        "date to a PhD thesis from 1980 :cite:`Speelpenning.1980`\n",
        "and were further developed in the late 1980s :cite:`Griewank.1989`.\n",
        "While backpropagation has become the default method\n",
        "for computing gradients, it is not the only option.\n",
        "For instance, the Julia programming language employs\n",
        "forward propagation :cite:`Revels.Lubin.Papamarkou.2016`.\n",
        "Before exploring methods,\n",
        "let's first master the autograd package.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "69f2c009",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T20:06:05.363043Z",
          "iopub.status.busy": "2023-08-18T20:06:05.362156Z",
          "iopub.status.idle": "2023-08-18T20:06:08.248480Z",
          "shell.execute_reply": "2023-08-18T20:06:08.247542Z"
        },
        "origin_pos": 4,
        "tab": [
          "tensorflow"
        ],
        "id": "69f2c009"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67f09864",
      "metadata": {
        "origin_pos": 6,
        "id": "67f09864"
      },
      "source": [
        "## A Simple Function\n",
        "\n",
        "Let's assume that we are interested\n",
        "in (**differentiating the function\n",
        "$y = 2\\mathbf{x}^{\\top}\\mathbf{x}$\n",
        "with respect to the column vector $\\mathbf{x}$.**)\n",
        "To start, we assign `x` an initial value.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "4a6f60a4",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T20:06:08.252752Z",
          "iopub.status.busy": "2023-08-18T20:06:08.252007Z",
          "iopub.status.idle": "2023-08-18T20:06:11.382382Z",
          "shell.execute_reply": "2023-08-18T20:06:11.381543Z"
        },
        "origin_pos": 9,
        "tab": [
          "tensorflow"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4a6f60a4",
        "outputId": "f9a5371b-4c93-4578-ed7f-2b7c42ad88d0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(4,), dtype=float32, numpy=array([0., 1., 2., 3.], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "x = tf.range(4, dtype=tf.float32)\n",
        "x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ae1557e",
      "metadata": {
        "origin_pos": 11,
        "tab": [
          "tensorflow"
        ],
        "id": "4ae1557e"
      },
      "source": [
        "[**Before we calculate the gradient\n",
        "of $y$ with respect to $\\mathbf{x}$,\n",
        "we need a place to store it.**]\n",
        "In general, we avoid allocating new memory\n",
        "every time we take a derivative\n",
        "because deep learning requires\n",
        "successively computing derivatives\n",
        "with respect to the same parameters\n",
        "a great many times,\n",
        "and we might risk running out of memory.\n",
        "Note that the gradient of a scalar-valued function\n",
        "with respect to a vector $\\mathbf{x}$\n",
        "is vector-valued with\n",
        "the same shape as $\\mathbf{x}$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "96dbfaf0",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T20:06:11.387537Z",
          "iopub.status.busy": "2023-08-18T20:06:11.386975Z",
          "iopub.status.idle": "2023-08-18T20:06:11.393377Z",
          "shell.execute_reply": "2023-08-18T20:06:11.392598Z"
        },
        "origin_pos": 14,
        "tab": [
          "tensorflow"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96dbfaf0",
        "outputId": "5ae1a4f1-f299-4aae-d44f-5bc90a2378fd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float32, numpy=1.0>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "x = tf.Variable(x) # Variable è Modificabile, Tensor è Non Modificabile.\n",
        "# si usano Variable per i pesi, aggiornati via backpropagation\n",
        "x[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "596f9a79",
      "metadata": {
        "origin_pos": 15,
        "id": "596f9a79"
      },
      "source": [
        "(**We now calculate our function of `x` and assign the result to `y`.**)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "b7cb5a95",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T20:06:11.397819Z",
          "iopub.status.busy": "2023-08-18T20:06:11.397313Z",
          "iopub.status.idle": "2023-08-18T20:06:11.410498Z",
          "shell.execute_reply": "2023-08-18T20:06:11.409672Z"
        },
        "origin_pos": 18,
        "tab": [
          "tensorflow"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7cb5a95",
        "outputId": "020fc887-edc7-4f8e-d970-daab24b58c29"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float32, numpy=28.0>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "# Record all computations onto a tape\n",
        "with tf.GradientTape() as t:\n",
        "    y = 2 * tf.tensordot(x, x, axes=1)\n",
        "y"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6bdc88ca",
      "metadata": {
        "origin_pos": 22,
        "tab": [
          "tensorflow"
        ],
        "id": "6bdc88ca"
      },
      "source": [
        "[**We can now calculate the gradient of `y`\n",
        "with respect to `x`**] by calling\n",
        "the `gradient` method.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "ca577b1d",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T20:06:11.414882Z",
          "iopub.status.busy": "2023-08-18T20:06:11.414346Z",
          "iopub.status.idle": "2023-08-18T20:06:11.425808Z",
          "shell.execute_reply": "2023-08-18T20:06:11.425045Z"
        },
        "origin_pos": 26,
        "tab": [
          "tensorflow"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ca577b1d",
        "outputId": "98ed180f-5b58-42c9-999e-f85f8876f928"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(4,), dtype=float32, numpy=array([ 0.,  4.,  8., 12.], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "x_grad = t.gradient(y, x)\n",
        "x_grad # per default si calcola una volta per sempre, e non si può ricalcolare (ex. per derivate seconde)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9abbdc52",
      "metadata": {
        "origin_pos": 28,
        "id": "9abbdc52"
      },
      "source": [
        "(**We already know that the gradient of the function $y = 2\\mathbf{x}^{\\top}\\mathbf{x}$\n",
        "with respect to $\\mathbf{x}$ should be $4\\mathbf{x}$.**)\n",
        "We can now verify that the automatic gradient computation\n",
        "and the expected result are identical.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "64264b1d",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T20:06:11.430333Z",
          "iopub.status.busy": "2023-08-18T20:06:11.429690Z",
          "iopub.status.idle": "2023-08-18T20:06:11.436204Z",
          "shell.execute_reply": "2023-08-18T20:06:11.435431Z"
        },
        "origin_pos": 31,
        "tab": [
          "tensorflow"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64264b1d",
        "outputId": "485e8496-9aab-42a0-9190-0a52453bb8d9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(4,), dtype=bool, numpy=array([ True,  True,  True,  True])>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "x_grad == 4 * x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad07be37",
      "metadata": {
        "origin_pos": 35,
        "tab": [
          "tensorflow"
        ],
        "id": "ad07be37"
      },
      "source": [
        "[**Now let's calculate\n",
        "another function of `x`\n",
        "and take its gradient.**]\n",
        "Note that TensorFlow resets the gradient buffer\n",
        "whenever we record a new gradient.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "4f6191bc",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T20:06:11.440829Z",
          "iopub.status.busy": "2023-08-18T20:06:11.440195Z",
          "iopub.status.idle": "2023-08-18T20:06:11.448608Z",
          "shell.execute_reply": "2023-08-18T20:06:11.447862Z"
        },
        "origin_pos": 38,
        "tab": [
          "tensorflow"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4f6191bc",
        "outputId": "a8e65cdb-b7fa-4973-efe2-2c70d7d76a2a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(4,), dtype=float32, numpy=array([1., 1., 1., 1.], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "with tf.GradientTape() as t:\n",
        "    y = tf.reduce_sum(x)\n",
        "t.gradient(y, x)  # Overwritten by the newly calculated gradient"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2267662f",
      "metadata": {
        "origin_pos": 40,
        "id": "2267662f"
      },
      "source": [
        "## Backward for Non-Scalar Variables\n",
        "\n",
        "When `y` is a vector,\n",
        "the most natural representation\n",
        "of the derivative of  `y`\n",
        "with respect to a vector `x`\n",
        "is a matrix called the *Jacobian*\n",
        "that contains the partial derivatives\n",
        "of each component of `y`\n",
        "with respect to each component of `x`.\n",
        "Likewise, for higher-order `y` and `x`,\n",
        "the result of differentiation could be an even higher-order tensor.\n",
        "\n",
        "While Jacobians do show up in some\n",
        "advanced machine learning techniques,\n",
        "more commonly we want to sum up\n",
        "the gradients of each component of `y`\n",
        "with respect to the full vector `x`,\n",
        "yielding a vector of the same shape as `x`.\n",
        "For example, we often have a vector\n",
        "representing the value of our loss function\n",
        "calculated separately for each example among\n",
        "a *batch* of training examples.\n",
        "Here, we just want to (**sum up the gradients\n",
        "computed individually for each example**).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e50c75c",
      "metadata": {
        "origin_pos": 43,
        "tab": [
          "tensorflow"
        ],
        "id": "8e50c75c"
      },
      "source": [
        "By default, TensorFlow returns the gradient of the sum.\n",
        "In other words, rather than returning\n",
        "the Jacobian $\\partial_{\\mathbf{x}} \\mathbf{y}$,\n",
        "it returns the gradient of the sum\n",
        "$\\partial_{\\mathbf{x}} \\sum_i y_i$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "ef1a3a1c",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T20:06:11.453205Z",
          "iopub.status.busy": "2023-08-18T20:06:11.452573Z",
          "iopub.status.idle": "2023-08-18T20:06:11.460303Z",
          "shell.execute_reply": "2023-08-18T20:06:11.459535Z"
        },
        "origin_pos": 46,
        "tab": [
          "tensorflow"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ef1a3a1c",
        "outputId": "3eacab12-b7ea-4030-8fe4-58c89763a583"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(4,), dtype=float32, numpy=array([0., 2., 4., 6.], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "with tf.GradientTape() as t:\n",
        "    y = x * x\n",
        "t.gradient(y, x)  # Same as y = tf.reduce_sum(x * x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6bcc0f8",
      "metadata": {
        "origin_pos": 48,
        "id": "c6bcc0f8"
      },
      "source": [
        "## Detaching Computation\n",
        "\n",
        "Sometimes, we wish to [**move some calculations\n",
        "outside of the recorded computational graph.**]\n",
        "For example, say that we use the input\n",
        "to create some auxiliary intermediate terms\n",
        "for which we do not want to compute a gradient.\n",
        "In this case, we need to *detach*\n",
        "the respective computational graph\n",
        "from the final result.\n",
        "The following toy example makes this clearer:\n",
        "suppose we have `z = x * y` and `y = x * x`\n",
        "but we want to focus on the *direct* influence of `x` on `z`\n",
        "rather than the influence conveyed via `y`.\n",
        "In this case, we can create a new variable `u`\n",
        "that takes the same value as `y`\n",
        "but whose *provenance* (how it was created)\n",
        "has been wiped out.\n",
        "Thus `u` has no ancestors in the graph\n",
        "and gradients do not flow through `u` to `x`.\n",
        "For example, taking the gradient of `z = x * u`\n",
        "will yield the result `u`,\n",
        "(not `3 * x * x` as you might have\n",
        "expected since `z = x * x * x`).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "51bd574b",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T20:06:11.464783Z",
          "iopub.status.busy": "2023-08-18T20:06:11.464171Z",
          "iopub.status.idle": "2023-08-18T20:06:11.472266Z",
          "shell.execute_reply": "2023-08-18T20:06:11.471510Z"
        },
        "origin_pos": 51,
        "tab": [
          "tensorflow"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51bd574b",
        "outputId": "03166c4c-a43b-4980-cbbb-f99cdfee5ce1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(4,), dtype=bool, numpy=array([ True,  True,  True,  True])>"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "# Set persistent=True to preserve the compute graph. Rimane a disposizione ad esempio per derivate seconde\n",
        "# This lets us run t.gradient more than once\n",
        "with tf.GradientTape(persistent=True) as t:\n",
        "    y = x * x\n",
        "    u = tf.stop_gradient(y)\n",
        "    z = u * x\n",
        "\n",
        "x_grad = t.gradient(z, x)\n",
        "x_grad == u"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3bb5edce",
      "metadata": {
        "origin_pos": 53,
        "id": "3bb5edce"
      },
      "source": [
        "Note that while this procedure\n",
        "detaches `y`'s ancestors\n",
        "from the graph leading to `z`,\n",
        "the computational graph leading to `y`\n",
        "persists and thus we can calculate\n",
        "the gradient of `y` with respect to `x`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "89c06f84",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T20:06:11.476639Z",
          "iopub.status.busy": "2023-08-18T20:06:11.476047Z",
          "iopub.status.idle": "2023-08-18T20:06:11.482151Z",
          "shell.execute_reply": "2023-08-18T20:06:11.481386Z"
        },
        "origin_pos": 56,
        "tab": [
          "tensorflow"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89c06f84",
        "outputId": "8c48182c-c0f4-402f-b951-20bc042226de"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(4,), dtype=bool, numpy=array([ True,  True,  True,  True])>"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "t.gradient(y, x) == 2 * x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e27bd5f0",
      "metadata": {
        "origin_pos": 58,
        "id": "e27bd5f0"
      },
      "source": [
        "## Gradients and Python Control Flow\n",
        "\n",
        "So far we reviewed cases where the path from input to output\n",
        "was well defined via a function such as `z = x * x * x`.\n",
        "Programming offers us a lot more freedom in how we compute results.\n",
        "For instance, we can make them depend on auxiliary variables\n",
        "or condition choices on intermediate results.\n",
        "One benefit of using automatic differentiation\n",
        "is that [**even if**] building the computational graph of\n",
        "(**a function required passing through a maze of Python control flow**)\n",
        "(e.g., conditionals, loops, and arbitrary function calls),\n",
        "(**we can still calculate the gradient of the resulting variable.**)\n",
        "To illustrate this, consider the following code snippet where\n",
        "the number of iterations of the `while` loop\n",
        "and the evaluation of the `if` statement\n",
        "both depend on the value of the input `a`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Computational Graph *Dinamico*\n"
      ],
      "metadata": {
        "id": "wt_xZq0bpISv"
      },
      "id": "wt_xZq0bpISv"
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "01db7884",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T20:06:11.486611Z",
          "iopub.status.busy": "2023-08-18T20:06:11.485965Z",
          "iopub.status.idle": "2023-08-18T20:06:11.490247Z",
          "shell.execute_reply": "2023-08-18T20:06:11.489476Z"
        },
        "origin_pos": 61,
        "tab": [
          "tensorflow"
        ],
        "id": "01db7884"
      },
      "outputs": [],
      "source": [
        "def f(a):\n",
        "    b = a * 2\n",
        "    while tf.norm(b) < 1000:\n",
        "        b = b * 2\n",
        "    if tf.reduce_sum(b) > 0:\n",
        "        c = b\n",
        "    else:\n",
        "        c = 100 * b\n",
        "    return c"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a=100.0\n",
        "%time f(a)\n",
        "%time f(a)\n",
        "%time f(a)\n",
        "%time f(a)\n",
        "%time f(a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHlBD7ZBoH2H",
        "outputId": "11e23dc8-97ab-4117-a94e-b6919cee4586"
      },
      "id": "sHlBD7ZBoH2H",
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 5.46 ms, sys: 49 µs, total: 5.51 ms\n",
            "Wall time: 6.08 ms\n",
            "CPU times: user 2.16 ms, sys: 17 µs, total: 2.18 ms\n",
            "Wall time: 2.17 ms\n",
            "CPU times: user 1.83 ms, sys: 0 ns, total: 1.83 ms\n",
            "Wall time: 1.84 ms\n",
            "CPU times: user 2.07 ms, sys: 0 ns, total: 2.07 ms\n",
            "Wall time: 2.11 ms\n",
            "CPU times: user 2.01 ms, sys: 0 ns, total: 2.01 ms\n",
            "Wall time: 2.02 ms\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1600.0"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b2e3749",
      "metadata": {
        "origin_pos": 63,
        "id": "4b2e3749"
      },
      "source": [
        "Below, we call this function, passing in a random value, as input.\n",
        "Since the input is a random variable,\n",
        "we do not know what form\n",
        "the computational graph will take.\n",
        "However, whenever we execute `f(a)`\n",
        "on a specific input, we realize\n",
        "a specific computational graph\n",
        "and can subsequently run `backward`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "b0ae9863",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T20:06:11.494696Z",
          "iopub.status.busy": "2023-08-18T20:06:11.494106Z",
          "iopub.status.idle": "2023-08-18T20:06:11.521514Z",
          "shell.execute_reply": "2023-08-18T20:06:11.520736Z"
        },
        "origin_pos": 66,
        "tab": [
          "tensorflow"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0ae9863",
        "outputId": "925285be-50ed-4ec2-f2a5-9c56956b2c1d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float32, numpy=2048.0>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "a = tf.Variable(tf.random.normal(shape=()))\n",
        "with tf.GradientTape() as t:\n",
        "    d = f(a)\n",
        "d_grad = t.gradient(d, a)\n",
        "d_grad"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b527579",
      "metadata": {
        "origin_pos": 68,
        "id": "0b527579"
      },
      "source": [
        "Even though our function `f` is, for demonstration purposes, a bit contrived,\n",
        "its dependence on the input is quite simple:\n",
        "it is a *linear* function of `a`\n",
        "with piecewise defined scale.\n",
        "As such, `f(a) / a` is a vector of constant entries\n",
        "and, moreover, `f(a) / a` needs to match\n",
        "the gradient of `f(a)` with respect to `a`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "17bdac35",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T20:06:11.525328Z",
          "iopub.status.busy": "2023-08-18T20:06:11.524694Z",
          "iopub.status.idle": "2023-08-18T20:06:11.531377Z",
          "shell.execute_reply": "2023-08-18T20:06:11.530587Z"
        },
        "origin_pos": 71,
        "tab": [
          "tensorflow"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17bdac35",
        "outputId": "0c518c6e-0687-47fa-a13d-3ed6940b85e9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=bool, numpy=True>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "d_grad == d / a"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "required_libs": [],
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}